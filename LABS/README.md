# ğŸ“Œ Q-Learning & Deep Q-Network (DQN) sur FrozenLake

Ce projet implÃ©mente diffÃ©rentes mÃ©thodes d'apprentissage par renforcement sur l'environnement **FrozenLake** de OpenAI Gym, avec une progression allant de la **Q-Table** Ã  des modÃ¨les plus avancÃ©s comme le **Q-Network** et le **Double DQN**.

## ğŸ“‚ Contenu du Projet

- `part0_qtable.py` â†’ ImplÃ©mentation classique de **Q-Learning** avec une **Q-Table**.
- `part0_qnetwork.py` â†’ ImplÃ©mentation du **Q-Network** (utilisant un rÃ©seau de neurones au lieu d'une table).
- `part4_dqn.py` â†’ ImplÃ©mentation d'un **Double DQN**, une version amÃ©liorÃ©e du DQN pour Ã©viter certaines erreurs dâ€™apprentissage.
- `gridworld.py` â†’ Code pour gÃ©nÃ©rer un environnement personnalisÃ©, utilisÃ© dans le **lab4**.
- `requirements.txt` â†’ Liste des bibliothÃ¨ques Python nÃ©cessaires Ã  l'exÃ©cution du projet.
- `run_experiments.py` â†’ Script permettant d'exÃ©cuter les expÃ©riences dans l'ordre automatiquement.

---

## âš™ï¸ Installation & PrÃ©-requis

### ğŸ“¥ 1. Cloner le projet

```bash
git clone <lien-du-repo-github>
cd <nom-du-repo>
```

### ğŸ“¦ 2. Installer les dÃ©pendances

Le fichier `requirements.txt` contient toutes les bibliothÃ¨ques nÃ©cessaires.

#### ğŸ–¥ï¸ Pour Windows / Mac / Linux
```bash
pip install -r requirements.txt
```

Si vous utilisez **Anaconda**, crÃ©ez un environnement dÃ©diÃ© :
```bash
conda create --name rl_env python=3.9
conda activate rl_env
pip install -r requirements.txt
```

---

## ğŸš€ ExÃ©cution des scripts

### ğŸ”¹ **Q-Table (Q-Learning classique)**

Ce script utilise une **table de valeurs Q** pour entraÃ®ner un agent Ã  naviguer dans une grille **4x4** de FrozenLake.

```bash
python part0_qtable.py
```

Ce modÃ¨le est rapide et converge efficacement, car FrozenLake est un environnement simple.

---

### ğŸ”¹ **Q-Network (RÃ©seau de neurones)**

Ce script remplace la **Q-Table** par un **rÃ©seau de neurones** qui approxime les valeurs dâ€™action.

```bash
python part0_qnetwork.py
```

ğŸ“Œ **Attention :** Le Q-Network **prend plus de temps** Ã  s'entraÃ®ner que la Q-Table car il doit ajuster les poids dâ€™un modÃ¨le neuronal.

---

### ğŸ”¹ **Lab 4 - Double DQN (Apprentissage avancÃ©)**

Ce script implÃ©mente un **Double DQN**, une version amÃ©liorÃ©e du DQN classique qui rÃ©duit le biais d'optimisme des valeurs Q.

```bash
python part4_dqn.py
```

ğŸ“Œ **Important :**
- L'entraÃ®nement est **beaucoup plus long**, car il utilise un rÃ©seau de neurones profond et un buffer dâ€™expÃ©rience.
- L'environnement utilisÃ© est **gridworld.py**, une grille personnalisÃ©e oÃ¹ lâ€™agent doit naviguer en Ã©vitant des obstacles.

---

### ğŸ”¹ **ExÃ©cution Automatique des ExpÃ©riences**

PlutÃ´t que d'exÃ©cuter les scripts un par un, vous pouvez utiliser le script `run_experiments.py`, qui installe les dÃ©pendances et exÃ©cute chaque script dans l'ordre.

```bash
python run_experiments.py
```

ğŸ“Œ **Ne fermez pas les fenÃªtres des interfaces affichÃ©es lors de l'exÃ©cution des scripts !**
- Les fermer peut **provoquer des erreurs** ou **arrÃªter le programme**.
- Si vous utilisez `run_experiments.py`, **laissez-le s'exÃ©cuter jusqu'au bout**. Si vous interrompez un algorithme pour passer au suivant, cela risque de ne pas fonctionner correctement.

---

## â³ Temps d'exÃ©cution estimÃ©

- **Q-Table** â†’ âš¡ **Rapide** (~ quelques secondes/minutes)
- **Q-Network** â†’ âš¡ **Rapide** (~ quelques secondes/minutes)
- **Double DQN (Lab 4)** â†’ ğŸ•’ **Long** (~ plusieurs minutes/heures, selon les paramÃ¨tres)

---

## ğŸ“ Conclusion

Ce projet illustre comment un agent peut apprendre Ã  **prendre des dÃ©cisions intelligentes** en utilisant diffÃ©rentes approches dâ€™apprentissage par renforcement.

La **Q-Table** est efficace pour les petits environnements, tandis que le **Q-Network** et le **Double DQN** permettent dâ€™Ã©tendre cette approche Ã  des scÃ©narios plus complexes.

Bon entraÃ®nement ! ğŸš€

